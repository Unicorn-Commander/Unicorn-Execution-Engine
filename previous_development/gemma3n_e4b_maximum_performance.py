#!/usr/bin/env python3
"""
Gemma 3n E4B Maximum Performance Implementation
Comprehensive solution for 20-50 TPS target performance
"""

import time
import torch
import torch.nn as nn
import logging
import psutil
import gc
from typing import Dict, Any, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MaximumPerformanceGemma3n:
    """Maximum performance Gemma 3n E4B with all optimizations"""
    
    def __init__(self, model_path: str = "./models/gemma-3n-e4b-it"):
        self.model_path = model_path
        self.tokenizer = None
        self.model = None
        
        # Performance tracking
        self.baseline_tps = 3.5
        self.target_tps = 20.0
        
        # Apply all optimizations
        self._apply_system_optimizations()
        self._load_quantized_model()
        self._apply_inference_optimizations()
        
    def _apply_system_optimizations(self):
        """Apply all system-level optimizations"""
        logger.info("üîß Applying system optimizations...")
        
        # CPU optimizations
        torch.set_num_threads(psutil.cpu_count())
        torch.set_num_interop_threads(psutil.cpu_count())
        
        # Memory optimizations
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        
        # Garbage collection
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        logger.info(f"‚úÖ System optimizations applied")
        logger.info(f"   CPU threads: {torch.get_num_threads()}")
        logger.info(f"   Interop threads: {torch.get_num_interop_threads()}")
        
    def _load_quantized_model(self):
        """Load model with quantization for maximum performance"""
        logger.info("üöÄ Loading model with quantization...")
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, 
            trust_remote_code=True
        )
        
        # Quantization configuration for maximum speed
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
            llm_int8_enable_fp32_cpu_offload=True
        )
        
        # Try different loading strategies for best performance
        loading_strategies = [
            {
                "name": "8-bit quantized",
                "kwargs": {
                    "quantization_config": quantization_config,
                    "torch_dtype": torch.float16,
                    "device_map": "auto",
                    "trust_remote_code": True,
                    "low_cpu_mem_usage": True
                }
            },
            {
                "name": "bfloat16 optimized",
                "kwargs": {
                    "torch_dtype": torch.bfloat16,
                    "device_map": "cpu",
                    "trust_remote_code": True,
                    "low_cpu_mem_usage": True,
                    "attn_implementation": "eager"
                }
            },
            {
                "name": "float16 optimized",
                "kwargs": {
                    "torch_dtype": torch.float16,
                    "device_map": "cpu",
                    "trust_remote_code": True,
                    "low_cpu_mem_usage": True
                }
            }
        ]
        
        best_model = None
        best_tps = 0
        best_config = None
        
        for strategy in loading_strategies:
            try:
                logger.info(f"üî¨ Testing: {strategy['name']}")
                
                start_time = time.time()
                model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    **strategy["kwargs"]
                )
                load_time = time.time() - start_time
                
                # Quick performance test
                tps = self._quick_performance_test(model)
                
                logger.info(f"   Load time: {load_time:.1f}s")
                logger.info(f"   Test TPS: {tps:.1f}")
                
                if tps > best_tps:
                    if best_model is not None:
                        del best_model
                    best_model = model
                    best_tps = tps
                    best_config = strategy["name"]
                else:
                    del model
                    
                # Clean up
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                logger.warning(f"   ‚ùå Failed: {e}")
                continue
        
        if best_model is None:
            raise RuntimeError("‚ùå No working model configuration found")
        
        self.model = best_model
        logger.info(f"‚úÖ Best configuration: {best_config}")
        logger.info(f"   Performance: {best_tps:.1f} TPS")
        
    def _quick_performance_test(self, model) -> float:
        """Quick performance test for model selection"""
        try:
            test_input = "Hello"
            inputs = self.tokenizer(test_input, return_tensors="pt")
            
            start_time = time.time()
            with torch.no_grad():
                outputs = model.generate(
                    inputs["input_ids"],
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id,
                    use_cache=True
                )
            
            generation_time = time.time() - start_time
            tokens_generated = len(outputs[0]) - len(inputs["input_ids"][0])
            
            return tokens_generated / generation_time if generation_time > 0 else 0
            
        except Exception:
            return 0
    
    def _apply_inference_optimizations(self):
        """Apply inference-specific optimizations"""
        logger.info("üîß Applying inference optimizations...")
        
        # Set model to evaluation mode
        self.model.eval()
        
        # Disable gradient computation globally
        torch.set_grad_enabled(False)
        
        # Try to compile model for faster inference (if supported)
        try:
            # Note: torch.compile may not be available in all PyTorch versions
            if hasattr(torch, 'compile'):
                logger.info("üöÄ Compiling model for maximum performance...")
                self.model = torch.compile(self.model, mode="max-autotune", fullgraph=True)
                logger.info("‚úÖ Model compiled successfully")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Model compilation failed: {e}")
            
        logger.info("‚úÖ Inference optimizations applied")
        
    def generate(self, prompt: str, max_tokens: int = 50, temperature: float = 0.7) -> Dict[str, Any]:
        """Generate text with maximum performance optimizations"""
        
        # Tokenize with optimizations
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt",
            truncation=True,
            padding=False,
            add_special_tokens=True
        )
        input_length = len(inputs["input_ids"][0])
        
        # Generation parameters optimized for speed
        generation_kwargs = {
            "max_new_tokens": max_tokens,
            "temperature": temperature,
            "do_sample": True,
            "top_p": 0.9,
            "top_k": 50,
            "pad_token_id": self.tokenizer.eos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "use_cache": True,
            "repetition_penalty": 1.05,
            "no_repeat_ngram_size": 3
        }
        
        # Generate with timing
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                inputs["input_ids"],
                **generation_kwargs
            )
        
        generation_time = time.time() - start_time
        
        # Decode response
        generated_tokens = outputs[0][input_length:]
        response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        # Calculate metrics
        tokens_generated = len(generated_tokens)
        tps = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            "response": response,
            "tokens_generated": tokens_generated,
            "generation_time": generation_time,
            "tokens_per_second": tps,
            "input_tokens": input_length,
            "total_tokens": input_length + tokens_generated,
            "performance_improvement": (tps / self.baseline_tps - 1) * 100 if self.baseline_tps > 0 else 0,
            "target_achievement": (tps / self.target_tps) * 100
        }
    
    def benchmark(self, num_tests: int = 5) -> Dict[str, Any]:
        """Run comprehensive benchmark"""
        logger.info(f"üîç Running comprehensive benchmark ({num_tests} tests)...")
        
        test_prompts = [
            "Hello",
            "Hello, how are you?",
            "Hello, I'm Aaron. Please tell me about yourself.",
            "Explain how neural networks work.",
            "Write a detailed analysis of artificial intelligence trends."
        ]
        
        all_results = []
        
        for i in range(num_tests):
            prompt = test_prompts[i % len(test_prompts)]
            logger.info(f"üîç Test {i+1}: '{prompt[:30]}...'")
            
            result = self.generate(prompt, max_tokens=30)
            all_results.append(result)
            
            logger.info(f"   TPS: {result['tokens_per_second']:.1f}")
            logger.info(f"   Response: {result['response'][:40]}...")
        
        # Calculate statistics
        tps_values = [r['tokens_per_second'] for r in all_results]
        avg_tps = sum(tps_values) / len(tps_values)
        min_tps = min(tps_values)
        max_tps = max(tps_values)
        
        improvement = (avg_tps / self.baseline_tps - 1) * 100 if self.baseline_tps > 0 else 0
        target_achievement = (avg_tps / self.target_tps) * 100
        
        return {
            "average_tps": avg_tps,
            "min_tps": min_tps,
            "max_tps": max_tps,
            "baseline_tps": self.baseline_tps,
            "target_tps": self.target_tps,
            "improvement_percent": improvement,
            "target_achievement_percent": target_achievement,
            "all_results": all_results
        }

def main():
    """Main function to test maximum performance"""
    logger.info("ü¶Ñ Gemma 3n E4B Maximum Performance Test")
    logger.info("=" * 60)
    
    try:
        # Create maximum performance model
        model = MaximumPerformanceGemma3n()
        
        # Run benchmark
        benchmark_results = model.benchmark(num_tests=3)
        
        # Display results
        logger.info("\nüìä BENCHMARK RESULTS:")
        logger.info("=" * 40)
        logger.info(f"   Average TPS: {benchmark_results['average_tps']:.1f}")
        logger.info(f"   Min TPS: {benchmark_results['min_tps']:.1f}")
        logger.info(f"   Max TPS: {benchmark_results['max_tps']:.1f}")
        logger.info(f"   Baseline TPS: {benchmark_results['baseline_tps']:.1f}")
        logger.info(f"   Target TPS: {benchmark_results['target_tps']:.1f}")
        
        # Performance analysis
        improvement = benchmark_results['improvement_percent']
        target_achievement = benchmark_results['target_achievement_percent']
        
        logger.info(f"\nüéØ PERFORMANCE ANALYSIS:")
        logger.info(f"   Improvement: {improvement:+.1f}% over baseline")
        logger.info(f"   Target achievement: {target_achievement:.1f}%")
        
        if target_achievement >= 100:
            logger.info("üéâ TARGET ACHIEVED! Performance is excellent!")
        elif target_achievement >= 50:
            logger.info("‚úÖ GOOD PROGRESS: Halfway to target")
        elif improvement > 0:
            logger.info("‚ö†Ô∏è  SOME IMPROVEMENT: But more optimization needed")
        else:
            logger.warning("‚ùå NO IMPROVEMENT: Need different approach")
            
        # Recommendations
        logger.info(f"\nüí° RECOMMENDATIONS:")
        avg_tps = benchmark_results['average_tps']
        
        if avg_tps < 5:
            logger.info("   - Consider smaller model variant")
            logger.info("   - Use aggressive quantization (4-bit)")
            logger.info("   - Implement model pruning")
        elif avg_tps < 10:
            logger.info("   - Try mixed precision inference")
            logger.info("   - Implement attention optimization")
            logger.info("   - Use speculative decoding")
        elif avg_tps < 20:
            logger.info("   - Implement KV cache optimization")
            logger.info("   - Try batch processing")
            logger.info("   - Consider model distillation")
        else:
            logger.info("   - Performance is excellent!")
            logger.info("   - Consider production deployment")
            
    except Exception as e:
        logger.error(f"‚ùå Maximum performance test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()