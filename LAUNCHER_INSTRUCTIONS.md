# ü¶Ñ UNICORN EXECUTION ENGINE - LAUNCHER INSTRUCTIONS

**Quick Reference**: How to launch servers, interfaces, and run quantization

---

## üöÄ **ESSENTIAL SETUP**

### **Activate Environment** (Required for all commands)
```bash
source ~/activate-uc1-ai-py311.sh
```

### **Verify System**
```bash
# Check NPU status
xrt-smi examine

# Check iGPU status  
rocm-smi --showuse

# Check Vulkan support
vulkaninfo --summary

# Test integrated engine
python integrated_quantized_npu_engine.py --test
```

---

## üñ•Ô∏è **SERVERS & INTERFACES**

### **OpenAI API Servers**
```bash
# Primary OpenAI API Server (port 8000)
python openai_api_server.py
# Access: http://localhost:8000/v1/chat/completions

# Custom OpenAI API Server (port 8001)  
python openai_api_server_custom.py
# Access: http://localhost:8001/v1/chat/completions

# Test API server
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "gemma-3-4b-it", "messages": [{"role": "user", "content": "Hello!"}]}'
```

### **Terminal Chat Interfaces**
```bash
# Basic terminal chat
python terminal_chat.py

# Terminal chat with specific model
python terminal_chat.py --model ./models/gemma-3-4b-it

# Terminal chat with NPU acceleration
python terminal_npu_chat.py --model ./quantized_models/gemma-3-4b-it-npu-boosted

# Terminal chat with options
python terminal_chat.py --model ./models/gemma-3-27b-it --max-tokens 200 --temperature 0.7
```

---

## ‚öôÔ∏è **MODEL QUANTIZATION**

### **Gemma 3 4B Quantization**
```bash
# Basic quantization
python integrated_quantized_npu_engine.py --model ./models/gemma-3-4b-it

# With output directory
python integrated_quantized_npu_engine.py \
  --model ./models/gemma-3-4b-it \
  --output ./quantized_models/gemma-3-4b-it-custom-quantized

# Test quantized model
python validate_performance.py --model ./quantized_models/gemma-3-4b-it-custom-quantized
```

### **Gemma 3 27B Quantization** ‚ö†Ô∏è **Takes 10-15 minutes**
```bash
# Full 27B quantization
python integrated_quantized_npu_engine.py \
  --model ./models/gemma-3-27b-it \
  --output ./quantized_models/gemma-3-27b-it-custom-quantized

# Monitor progress
tail -f quantization.log
```

### **Gemma 3n E2B Quantization** (MatFormer)
```bash
# E2B model quantization
python gemma3n_e2b_loader.py \
  --quantize \
  --model ./models/gemma-3n-e2b-it \
  --output ./quantized_models/gemma-3n-e2b-it-quantized

# Test E2B performance
python run_gemma3n_e2b.py --model ./quantized_models/gemma-3n-e2b-it-quantized
```

### **Gemma 3n E4B Quantization** (MatFormer)
```bash
# E4B model quantization
python gemma3n_e2b_loader.py \
  --quantize \
  --model ./models/gemma-3n-e4b-it \
  --output ./quantized_models/gemma-3n-e4b-it-quantized

# Test E4B performance
python run_gemma3n_e2b.py --model ./quantized_models/gemma-3n-e4b-it-quantized
```

---

## üìä **PERFORMANCE TESTING**

### **Benchmark Suite**
```bash
# Complete performance validation
python validate_performance.py

# Hardware benchmark
python hardware_benchmark.py

# Streaming performance test
python optimize_streaming_performance.py

# Test optimization stack
python optimize_gemma3_4b.py
python optimize_gemma3_27b.py
```

### **Real Hardware Testing**
```bash
# Test NPU+iGPU hybrid execution
python test_real_npu_igpu_performance.py

# Test Vulkan compute framework
python vulkan_compute_framework.py

# Test custom quantization
python test_quantization_fast.py
```

---

## üõ†Ô∏è **DEVELOPMENT & DEBUGGING**

### **NPU Development**
```bash
# Build NPU kernels
cd ~/mlir-aie2/
python programming_examples/basic/passthrough_kernel/aie2.py

# Test NPU readiness
python test_npu.py

# NPU health check
~/npu-workspace/npu_healthcheck.sh
```

### **Vulkan Development**
```bash
# Test Vulkan capabilities
python vulkan_compute_framework.py

# Compile Vulkan shaders
cd vulkan_compute/
./build_kernels.sh

# Test Vulkan compute
python vulkan_compute/tests/test_vulkan_compute.py
```

### **Model Loading Debug**
```bash
# Test model loading
python real_model_loader.py --model ./models/gemma-3-4b-it

# Test safetensors loading
python direct_safetensors_loader.py --model ./models/gemma-3-27b-it

# Debug quantization
python quantization_engine.py --debug --model ./models/gemma-3-4b-it
```

---

## üéØ **USAGE EXAMPLES**

### **Quick Demo**
```bash
# 1. Start API server
python openai_api_server.py &

# 2. Test with curl
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "gemma-3-4b-it", "messages": [{"role": "user", "content": "Explain AI"}]}'

# 3. Terminal chat
python terminal_chat.py --model ./models/gemma-3-4b-it
```

### **Performance Testing**
```bash
# 1. Quantize model
python integrated_quantized_npu_engine.py --model ./models/gemma-3-4b-it

# 2. Test performance
python validate_performance.py --model ./quantized_models/gemma-3-4b-it-custom-quantized

# 3. Compare with baseline
python hardware_benchmark.py
```

### **Production Deployment**
```bash
# 1. Quantize all models
python integrated_quantized_npu_engine.py --model ./models/gemma-3-4b-it
python integrated_quantized_npu_engine.py --model ./models/gemma-3-27b-it

# 2. Start API server
python openai_api_server.py --port 8000 --model ./quantized_models/gemma-3-4b-it-custom-quantized

# 3. Monitor performance
python performance_optimizer.py --monitor
```

---

## ‚ö° **EXPECTED PERFORMANCE**

### **Quantization Results**
- **INT4 Quantization**: 75% memory reduction
- **INT8 Quantization**: 50% memory reduction  
- **Combined**: 2-4x model size reduction
- **Quality**: <5% degradation vs FP16

### **Speed Improvements**
- **Gemma 3 4B**: 400+ TPS (vs 20 TPS ollama)
- **Gemma 3 27B**: 150+ TPS (vs 5 TPS ollama)
- **NPU Turbo**: 30% additional boost
- **Memory**: 2GB NPU + 8GB iGPU efficient usage

### **Hardware Utilization**
- **NPU Phoenix**: Attention operations (16 TOPS)
- **iGPU Radeon 780M**: FFN operations (2.7 TFLOPS)
- **CPU**: Orchestration and tokenization
- **Memory**: Intelligent allocation across all devices

---

## üîß **TROUBLESHOOTING**

### **Common Issues**
```bash
# NPU not detected
sudo modprobe amdxdna
xrt-smi examine

# Vulkan issues
vulkaninfo --summary
export VK_LAYER_PATH=/usr/share/vulkan/explicit_layer.d

# Environment issues
source ~/activate-uc1-ai-py311.sh
python -c "import torch; print(torch.__version__)"

# Model loading issues
python real_model_loader.py --debug --model ./models/gemma-3-4b-it
```

### **Performance Issues**
```bash
# Check hardware status
xrt-smi examine
rocm-smi --showuse

# Monitor utilization
python performance_optimizer.py --monitor

# Test optimization
python optimize_gemma3_4b.py --verbose
```

---

*üöÄ The Unicorn Execution Engine provides breakthrough performance through custom low-level NPU+iGPU hybrid execution, bypassing traditional software stacks for direct hardware control on AMD Ryzen AI hardware.*