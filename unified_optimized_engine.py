#!/usr/bin/env python3
"""
Unified Optimized NPU+iGPU Engine
Integration of all optimizations with hardware-specific tuning for:
- NPU Phoenix (16 TOPS) with turbo mode
- AMD Radeon 780M (12 CUs, 2.7 TFLOPS)
- 96GB DDR5-5600 HMA architecture
"""

import numpy as np
import time
import logging
import os
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import threading
import subprocess

# Import our optimized components
from npu_attention_kernel import NPUAttentionKernel, NPUAttentionConfig
from optimized_vulkan_compute import OptimizedVulkanCompute
from hma_zero_copy_optimization import OptimizedMemoryBridge
from kernel_fusion_optimization import KernelFusionOptimizer

logger = logging.getLogger(__name__)

@dataclass
class HardwareConfig:
    """Hardware-specific configuration for NPU Phoenix + Radeon 780M"""
    # NPU Phoenix specifications
    npu_tops: int = 16
    npu_memory_gb: int = 2
    npu_compute_units: int = 5
    npu_turbo_enabled: bool = True
    
    # AMD Radeon 780M specifications
    igpu_compute_units: int = 12
    igpu_tflops: float = 2.7
    igpu_memory_gb: int = 16
    igpu_architecture: str = "RDNA3"
    
    # Memory architecture
    ddr5_total_gb: int = 96
    ddr5_speed: str = "DDR5-5600"
    hma_enabled: bool = True
    
    # Performance tuning parameters
    npu_block_size: int = 64      # Optimized for Phoenix wavefront
    igpu_workgroup_x: int = 8     # RDNA3 optimal workgroup
    igpu_workgroup_y: int = 8
    igpu_tile_size: int = 16      # Memory coalescing
    cpu_threads: int = 16         # Ryzen 9 8945HS

class HardwareSpecificTuner:
    """Hardware-specific tuner for NPU Phoenix + Radeon 780M"""
    
    def __init__(self, hw_config: HardwareConfig):
        self.hw_config = hw_config
        self.tuning_cache = {}
        self.performance_history = []
        
    def detect_hardware_capabilities(self):
        """Detect and validate hardware capabilities"""
        logger.info("ğŸ” Detecting hardware capabilities...")
        
        capabilities = {
            'npu_detected': False,
            'npu_turbo': False,
            'igpu_detected': False,
            'igpu_vulkan': False,
            'memory_bandwidth': 0.0
        }
        
        # Detect NPU Phoenix
        try:
            result = subprocess.run([
                "/opt/xilinx/xrt/bin/xrt-smi", "examine"
            ], capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0:
                capabilities['npu_detected'] = True
                logger.info(f"   âœ… NPU Phoenix detected: {self.hw_config.npu_tops} TOPS")
                
                # Enable turbo mode
                turbo_result = subprocess.run([
                    "sudo", "/opt/xilinx/xrt/bin/xrt-smi", "configure", "--pmode", "turbo"
                ], capture_output=True, text=True, timeout=10)
                
                if turbo_result.returncode == 0:
                    capabilities['npu_turbo'] = True
                    logger.info("   ğŸš€ NPU turbo mode enabled")
                    
        except Exception as e:
            logger.warning(f"NPU detection failed: {e}")
        
        # Detect AMD Radeon 780M
        try:
            result = subprocess.run([
                "vulkaninfo", "--summary"
            ], capture_output=True, text=True, timeout=10)
            
            if "RADV PHOENIX" in result.stdout:
                capabilities['igpu_detected'] = True
                capabilities['igpu_vulkan'] = True
                logger.info(f"   âœ… AMD Radeon 780M detected: {self.hw_config.igpu_compute_units} CUs")
                
        except Exception as e:
            logger.warning(f"iGPU detection failed: {e}")
        
        # Measure memory bandwidth
        capabilities['memory_bandwidth'] = self._measure_memory_bandwidth()
        
        return capabilities
    
    def _measure_memory_bandwidth(self):
        """Measure actual DDR5 memory bandwidth"""
        logger.info("   ğŸ“Š Measuring memory bandwidth...")
        
        try:
            # Create large test arrays
            size_mb = 1024  # 1GB test
            test_data = np.random.randn(size_mb * 1024 * 1024 // 4).astype(np.float32)
            
            # Measure copy performance
            start_time = time.time()
            copy_data = np.copy(test_data)
            copy_time = time.time() - start_time
            
            bandwidth_gbps = (test_data.nbytes * 2) / (copy_time * 1e9)  # Read + Write
            logger.info(f"   ğŸ“Š Memory bandwidth: {bandwidth_gbps:.2f} GB/s")
            
            return bandwidth_gbps
            
        except Exception as e:
            logger.warning(f"Memory bandwidth test failed: {e}")
            return 89.6  # Theoretical DDR5-5600 bandwidth
    
    def tune_npu_parameters(self, model_config):
        """Tune NPU parameters for Phoenix architecture"""
        logger.info("ğŸ¯ Tuning NPU parameters for Phoenix...")
        
        # Phoenix-specific optimizations
        tuned_params = {
            'seq_block_size': min(self.hw_config.npu_block_size, model_config.get('seq_length', 2048)),
            'attention_heads_per_cu': model_config.get('num_heads', 32) // self.hw_config.npu_compute_units,
            'memory_allocation': int(self.hw_config.npu_memory_gb * 1024 * 0.8),  # 80% utilization
            'precision_mode': 'fp16_optimized',
            'pipeline_depth': 4,  # Phoenix optimal
            'turbo_frequency': 'max' if self.hw_config.npu_turbo_enabled else 'nominal'
        }
        
        logger.info(f"   ğŸ¯ NPU block size: {tuned_params['seq_block_size']}")
        logger.info(f"   ğŸ¯ Heads per CU: {tuned_params['attention_heads_per_cu']}")
        logger.info(f"   ğŸ¯ Memory allocation: {tuned_params['memory_allocation']}MB")
        
        return tuned_params
    
    def tune_igpu_parameters(self, model_config):
        """Tune iGPU parameters for RDNA3 architecture"""
        logger.info("ğŸ¯ Tuning iGPU parameters for RDNA3...")
        
        # RDNA3-specific optimizations
        tuned_params = {
            'workgroup_size_x': self.hw_config.igpu_workgroup_x,
            'workgroup_size_y': self.hw_config.igpu_workgroup_y,
            'tile_size': self.hw_config.igpu_tile_size,
            'compute_units_used': min(self.hw_config.igpu_compute_units, 10),  # Reserve 2 CUs
            'memory_allocation': int(self.hw_config.igpu_memory_gb * 1024 * 0.85),  # 85% utilization
            'precision_mode': 'fp32_block_fp16_accumulate',
            'wave_size': 64,  # RDNA3 wavefront
            'lds_size_kb': 64,  # Local data share per workgroup
            'async_compute': True
        }
        
        logger.info(f"   ğŸ¯ Workgroup: {tuned_params['workgroup_size_x']}x{tuned_params['workgroup_size_y']}")
        logger.info(f"   ğŸ¯ Tile size: {tuned_params['tile_size']}")
        logger.info(f"   ğŸ¯ CUs used: {tuned_params['compute_units_used']}")
        
        return tuned_params
    
    def tune_memory_parameters(self):
        """Tune HMA memory parameters"""
        logger.info("ğŸ¯ Tuning HMA memory parameters...")
        
        tuned_params = {
            'zero_copy_pools': {
                'attention_pool': int(self.hw_config.ddr5_total_gb * 0.15 * 1024 * 1024 * 1024),  # 15%
                'ffn_pool': int(self.hw_config.ddr5_total_gb * 0.25 * 1024 * 1024 * 1024),        # 25%
                'model_weights': int(self.hw_config.ddr5_total_gb * 0.35 * 1024 * 1024 * 1024),   # 35%
                'buffer_pool': int(self.hw_config.ddr5_total_gb * 0.10 * 1024 * 1024 * 1024)      # 10%
            },
            'prefetch_size_mb': 256,
            'cache_line_size': 64,
            'numa_aware': True,
            'page_size': '2MB',  # Huge pages
            'bandwidth_target_gbps': 85.0  # 95% of theoretical
        }
        
        logger.info(f"   ğŸ¯ Attention pool: {tuned_params['zero_copy_pools']['attention_pool'] // 1024 // 1024 // 1024}GB")
        logger.info(f"   ğŸ¯ FFN pool: {tuned_params['zero_copy_pools']['ffn_pool'] // 1024 // 1024 // 1024}GB")
        logger.info(f"   ğŸ¯ Model weights: {tuned_params['zero_copy_pools']['model_weights'] // 1024 // 1024 // 1024}GB")
        
        return tuned_params

class UnifiedOptimizedEngine:
    """Unified engine with all optimizations integrated"""
    
    def __init__(self, model_path: Optional[str] = None):
        # Hardware configuration
        self.hw_config = HardwareConfig()
        self.hw_tuner = HardwareSpecificTuner(self.hw_config)
        
        # Optimized components
        self.npu_kernel = None
        self.vulkan_compute = None
        self.memory_bridge = None
        self.kernel_fusion = None
        
        # Model configuration
        self.model_path = Path(model_path) if model_path else None
        self.model_config = {}
        self.model_weights = {}
        
        # Performance tracking
        self.performance_stats = {
            'total_inference_time': 0.0,
            'npu_time': 0.0,
            'igpu_time': 0.0,
            'memory_time': 0.0,
            'tokens_processed': 0,
            'average_tps': 0.0
        }
        
        self.initialized = False
        
        logger.info("ğŸ¦„ Unified Optimized Engine initialized")
        logger.info(f"   Target: NPU Phoenix {self.hw_config.npu_tops} TOPS + Radeon 780M {self.hw_config.igpu_tflops} TFLOPS")
    
    def initialize(self):
        """Initialize unified optimized engine"""
        logger.info("ğŸš€ Initializing Unified Optimized Engine...")
        
        try:
            # Step 1: Detect and validate hardware
            capabilities = self.hw_tuner.detect_hardware_capabilities()
            
            if not capabilities['npu_detected']:
                logger.warning("âš ï¸ NPU not detected, using simulation mode")
            if not capabilities['igpu_detected']:
                logger.warning("âš ï¸ iGPU not detected, using CPU fallback")
            
            # Step 2: Load model configuration
            if not self._load_model_config():
                logger.error("âŒ Failed to load model configuration")
                return False
            
            # Step 3: Hardware-specific tuning
            npu_params = self.hw_tuner.tune_npu_parameters(self.model_config)
            igpu_params = self.hw_tuner.tune_igpu_parameters(self.model_config)
            memory_params = self.hw_tuner.tune_memory_parameters()
            
            # Step 4: Initialize optimized components
            if not self._initialize_npu_kernel(npu_params):
                return False
            if not self._initialize_vulkan_compute(igpu_params):
                return False
            if not self._initialize_memory_bridge(memory_params):
                return False
            if not self._initialize_kernel_fusion():
                return False
            
            # Step 5: Load and optimize model weights
            if self.model_path and self.model_path.exists():
                if not self._load_optimized_weights():
                    logger.warning("âš ï¸ Model weights not loaded, using synthetic weights")
            
            self.initialized = True
            logger.info("âœ… Unified Optimized Engine ready!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Engine initialization failed: {e}")
            return False
    
    def _load_model_config(self):
        """Load model configuration"""
        logger.info("ğŸ“‹ Loading model configuration...")
        
        # Default Gemma 3 27B configuration
        self.model_config = {
            'model_name': 'gemma-3-27b-it',
            'vocab_size': 256000,
            'seq_length': 2048,
            'd_model': 4096,
            'num_layers': 62,
            'num_heads': 32,
            'intermediate_size': 14336,
            'head_dim': 128,
            'rms_norm_eps': 1e-6,
            'rope_theta': 10000.0
        }
        
        # Try to load from model directory
        if self.model_path:
            config_file = self.model_path / "config.json"
            if config_file.exists():
                try:
                    with open(config_file) as f:
                        loaded_config = json.load(f)
                    self.model_config.update(loaded_config)
                    logger.info(f"   âœ… Loaded config from {config_file}")
                except Exception as e:
                    logger.warning(f"Config loading failed: {e}")
        
        logger.info(f"   ğŸ“Š Model: {self.model_config['model_name']}")
        logger.info(f"   ğŸ“Š Layers: {self.model_config['num_layers']}")
        logger.info(f"   ğŸ“Š Parameters: ~27B")
        
        return True
    
    def _initialize_npu_kernel(self, npu_params):
        """Initialize NPU kernel with hardware tuning"""
        logger.info("ğŸ§  Initializing NPU kernel...")
        
        npu_config = NPUAttentionConfig(
            seq_length=self.model_config['seq_length'],
            d_model=self.model_config['d_model'],
            num_heads=self.model_config['num_heads'],
            npu_memory_mb=npu_params['memory_allocation'],
            precision=npu_params['precision_mode']
        )
        
        self.npu_kernel = NPUAttentionKernel(npu_config)
        return self.npu_kernel.initialize()
    
    def _initialize_vulkan_compute(self, igpu_params):
        """Initialize Vulkan compute with hardware tuning"""
        logger.info("ğŸ® Initializing Vulkan compute...")
        
        self.vulkan_compute = OptimizedVulkanCompute()
        
        # Apply RDNA3-specific parameters
        self.vulkan_compute.WORKGROUP_SIZE_X = igpu_params['workgroup_size_x']
        self.vulkan_compute.WORKGROUP_SIZE_Y = igpu_params['workgroup_size_y']
        self.vulkan_compute.TILE_SIZE = igpu_params['tile_size']
        
        return self.vulkan_compute.initialize()
    
    def _initialize_memory_bridge(self, memory_params):
        """Initialize memory bridge with HMA tuning"""
        logger.info("ğŸŒ‰ Initializing memory bridge...")
        
        self.memory_bridge = OptimizedMemoryBridge()
        
        # Configure zero-copy pools
        self.memory_bridge.zero_copy_pools = memory_params['zero_copy_pools']
        
        return self.memory_bridge.initialize()
    
    def _initialize_kernel_fusion(self):
        """Initialize kernel fusion"""
        logger.info("ğŸ”— Initializing kernel fusion...")
        
        self.kernel_fusion = KernelFusionOptimizer()
        return self.kernel_fusion.initialize()
    
    def _load_optimized_weights(self):
        """Load and optimize model weights"""
        logger.info("âš–ï¸ Loading optimized model weights...")
        
        try:
            # Check for quantized weights
            quantized_path = self.model_path / "quantized_weights.npz"
            if quantized_path.exists():
                self.model_weights = np.load(quantized_path, allow_pickle=True)
                logger.info(f"   âœ… Loaded quantized weights: {len(self.model_weights)} tensors")
                return True
            
            # Create synthetic optimized weights for testing
            logger.info("   ğŸ”§ Creating synthetic optimized weights...")
            self._create_synthetic_weights()
            return True
            
        except Exception as e:
            logger.error(f"Weight loading failed: {e}")
            return False
    
    def _create_synthetic_weights(self):
        """Create synthetic weights for testing"""
        d_model = self.model_config['d_model']
        intermediate_size = self.model_config['intermediate_size']
        
        # Create hardware-optimized synthetic weights
        self.model_weights = {
            'attention_weights': np.random.randn(d_model, d_model).astype(np.float16) * 0.02,
            'ffn_gate_weight': np.random.randn(d_model, intermediate_size).astype(np.float16) * 0.02,
            'ffn_up_weight': np.random.randn(d_model, intermediate_size).astype(np.float16) * 0.02,
            'ffn_down_weight': np.random.randn(intermediate_size, d_model).astype(np.float16) * 0.02
        }
        
        logger.info(f"   âœ… Created synthetic weights: {len(self.model_weights)} tensors")
    
    def execute_optimized_inference(self, input_tokens: List[int], max_new_tokens: int = 50):
        """Execute optimized inference with all optimizations"""
        if not self.initialized:
            raise RuntimeError("Engine not initialized")
        
        logger.info(f"ğŸš€ Executing optimized inference: {len(input_tokens)} input tokens â†’ {max_new_tokens} new tokens")
        
        total_start_time = time.time()
        
        # Initialize hidden states
        seq_len = len(input_tokens)
        d_model = self.model_config['d_model']
        hidden_states = np.random.randn(seq_len, d_model).astype(np.float32) * 0.1
        
        generated_tokens = []
        
        # Generation loop
        for token_idx in range(max_new_tokens):
            token_start_time = time.time()
            
            # Process through all layers with optimizations
            for layer_idx in range(self.model_config['num_layers']):
                layer_start = time.time()
                
                # NPU attention with hardware tuning
                npu_start = time.time()
                attention_output = self._execute_optimized_attention(hidden_states, layer_idx)
                npu_time = time.time() - npu_start
                
                # Zero-copy memory transfer
                memory_start = time.time()
                igpu_tensor, offset, size = self.memory_bridge.optimized_npu_to_igpu_transfer(
                    attention_output, "attention_output"
                )
                memory_time = time.time() - memory_start
                
                # iGPU FFN with kernel fusion
                igpu_start = time.time()
                ffn_output = self._execute_optimized_ffn(igpu_tensor, layer_idx)
                igpu_time = time.time() - igpu_start
                
                # Free memory
                self.memory_bridge.free_optimized_buffer("attention_pool", offset, size)
                
                # Update hidden states
                hidden_states = ffn_output
                
                # Update stats
                self.performance_stats['npu_time'] += npu_time
                self.performance_stats['igpu_time'] += igpu_time
                self.performance_stats['memory_time'] += memory_time
                
                if layer_idx % 10 == 0:
                    layer_time = time.time() - layer_start
                    logger.info(f"   Layer {layer_idx+1}/{self.model_config['num_layers']}: {layer_time*1000:.2f}ms")
            
            # Sample next token
            next_token = self._sample_next_token(hidden_states)
            generated_tokens.append(next_token)
            
            token_time = time.time() - token_start_time
            current_tps = (token_idx + 1) / (time.time() - total_start_time)
            
            if (token_idx + 1) % 10 == 0:
                logger.info(f"   Token {token_idx+1}/{max_new_tokens}: {token_time*1000:.2f}ms, TPS: {current_tps:.2f}")
        
        # Final statistics
        total_time = time.time() - total_start_time
        total_tokens = len(generated_tokens)
        final_tps = total_tokens / total_time
        
        self.performance_stats.update({
            'total_inference_time': total_time,
            'tokens_processed': total_tokens,
            'average_tps': final_tps
        })
        
        logger.info(f"âœ… Optimized inference completed:")
        logger.info(f"   Total time: {total_time:.2f}s")
        logger.info(f"   Generated tokens: {total_tokens}")
        logger.info(f"   Final TPS: {final_tps:.2f}")
        
        return generated_tokens
    
    def _execute_optimized_attention(self, hidden_states, layer_idx):
        """Execute optimized attention with NPU"""
        # Use hardware-tuned NPU kernel
        return self.npu_kernel.compute_attention(hidden_states, hidden_states, hidden_states)
    
    def _execute_optimized_ffn(self, hidden_states, layer_idx):
        """Execute optimized FFN with kernel fusion"""
        # Use kernel fusion for FFN operations
        return self.kernel_fusion.fuse_ffn_operations(
            hidden_states,
            self.model_weights.get('ffn_gate_weight', np.eye(hidden_states.shape[1])),
            self.model_weights.get('ffn_up_weight', np.eye(hidden_states.shape[1])),
            self.model_weights.get('ffn_down_weight', np.eye(hidden_states.shape[1]))
        )
    
    def _sample_next_token(self, hidden_states):
        """Sample next token from hidden states"""
        # Simple sampling for now
        return np.random.randint(0, self.model_config['vocab_size'])
    
    def get_performance_report(self):
        """Get comprehensive performance report"""
        stats = self.performance_stats.copy()
        
        if stats['tokens_processed'] > 0:
            stats['time_per_token_ms'] = (stats['total_inference_time'] / stats['tokens_processed']) * 1000
            stats['npu_utilization'] = (stats['npu_time'] / stats['total_inference_time']) * 100
            stats['igpu_utilization'] = (stats['igpu_time'] / stats['total_inference_time']) * 100
            stats['memory_overhead'] = (stats['memory_time'] / stats['total_inference_time']) * 100
        
        return stats

if __name__ == "__main__":
    # Test unified optimized engine
    logger.info("ğŸ§ª Testing Unified Optimized Engine...")
    
    engine = UnifiedOptimizedEngine()
    if engine.initialize():
        # Test inference
        input_tokens = [1, 2, 3, 4, 5, 6, 7, 8]
        generated_tokens = engine.execute_optimized_inference(input_tokens, max_new_tokens=20)
        
        # Performance report
        report = engine.get_performance_report()
        
        print(f"\nğŸ“Š Performance Report:")
        print(f"   Generated tokens: {len(generated_tokens)}")
        print(f"   Average TPS: {report['average_tps']:.2f}")
        print(f"   Time per token: {report.get('time_per_token_ms', 0):.2f}ms")
        print(f"   NPU utilization: {report.get('npu_utilization', 0):.1f}%")
        print(f"   iGPU utilization: {report.get('igpu_utilization', 0):.1f}%")
        print(f"   Memory overhead: {report.get('memory_overhead', 0):.1f}%")
        
        print(f"\nâœ… Unified engine test completed!")
    else:
        print("âŒ Engine initialization failed")