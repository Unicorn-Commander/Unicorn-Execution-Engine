#!/usr/bin/env python3
"""
Pure Hardware API Server with HMA GPU Memory Support
Loads model to VRAM/GTT instead of system RAM
"""

import os
import sys
import asyncio
import time
import logging
import traceback
import psutil
import subprocess
from typing import Dict, List, Optional, Any
from pathlib import Path

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn
import json
import uuid
import numpy as np

# Import our HMA-enabled pipeline
from pure_hardware_pipeline_hma import PureHardwarePipelineHMA

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set HMA environment variables
os.environ['HSA_ENABLE_UNIFIED_MEMORY'] = '1'
os.environ['HSA_FORCE_FINE_GRAIN_PCIE'] = '1'
os.environ['PYTORCH_HIP_ALLOC_CONF'] = 'expandable_segments:True'

# Pydantic models
class ChatMessage(BaseModel):
    role: str = Field(..., description="Role of the message sender")
    content: str = Field(..., description="Content of the message")

class ChatCompletionRequest(BaseModel):
    model: str = Field(default="gemma-3-27b-hma", description="Model to use")
    messages: List[ChatMessage] = Field(..., description="List of messages")
    max_tokens: Optional[int] = Field(default=100, description="Maximum tokens to generate")
    temperature: Optional[float] = Field(default=0.7, description="Sampling temperature")
    top_p: Optional[float] = Field(default=0.9, description="Top-p sampling")
    stream: Optional[bool] = Field(default=False, description="Stream the response")

# Global state
pipeline = None
model_loaded = False

# Simple tokenizer
class SimpleTokenizer:
    def encode(self, text: str, return_tensors: str = None) -> List[int]:
        tokens = [ord(c) % 32000 for c in text[:50]]
        return tokens
    
    def decode(self, tokens: List[int], skip_special_tokens: bool = True) -> str:
        return f"[Generated response using HMA GPU acceleration with {len(tokens)} tokens]"

tokenizer = SimpleTokenizer()

# FastAPI app
app = FastAPI(
    title="Pure Hardware Gemma 27B API with HMA",
    description="OpenAI v1 compatible API with HMA GPU memory (VRAM/GTT allocation)",
    version="2.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def get_gpu_memory_stats():
    """Get GPU memory statistics"""
    stats = {}
    
    try:
        # Try to get from sysfs
        from pathlib import Path
        gpu_paths = list(Path('/sys/class/drm').glob('card*/device'))
        
        for gpu_path in gpu_paths:
            vendor_path = gpu_path / 'vendor'
            if vendor_path.exists() and vendor_path.read_text().strip() == '0x1002':
                # AMD GPU
                vram_total = gpu_path / 'mem_info_vram_total'
                vram_used = gpu_path / 'mem_info_vram_used'
                gtt_total = gpu_path / 'mem_info_gtt_total'
                gtt_used = gpu_path / 'mem_info_gtt_used'
                
                if vram_total.exists():
                    stats['vram_total_gb'] = int(vram_total.read_text()) / (1024**3)
                    stats['vram_used_gb'] = int(vram_used.read_text()) / (1024**3) if vram_used.exists() else 0
                
                if gtt_total.exists():
                    stats['gtt_total_gb'] = int(gtt_total.read_text()) / (1024**3)
                    stats['gtt_used_gb'] = int(gtt_used.read_text()) / (1024**3) if gtt_used.exists() else 0
                
                break
    except:
        pass
    
    return stats

def get_memory_distribution():
    """Get model memory distribution across VRAM/GTT/RAM"""
    if not pipeline:
        return {}
    
    distribution = {
        'vram_layers': len(getattr(pipeline, 'vram_tensors', {})),
        'gtt_layers': len(getattr(pipeline, 'gtt_tensors', {})),
        'cpu_layers': len(getattr(pipeline, 'cpu_tensors', {})),
        'total_layers': 62
    }
    
    return distribution

async def load_model():
    """Load model using HMA pipeline"""
    global pipeline, model_loaded
    
    try:
        logger.info("üöÄ STARTING HMA GPU MODEL LOADING")
        logger.info("üíæ Model will be distributed across VRAM/GTT/RAM")
        
        # Show initial GPU memory state
        gpu_stats = get_gpu_memory_stats()
        if gpu_stats:
            logger.info(f"üìä Initial GPU Memory:")
            logger.info(f"   VRAM: {gpu_stats.get('vram_used_gb', 0):.1f}/{gpu_stats.get('vram_total_gb', 0):.1f}GB")
            logger.info(f"   GTT: {gpu_stats.get('gtt_used_gb', 0):.1f}/{gpu_stats.get('gtt_total_gb', 0):.1f}GB")
        
        model_path = "./quantized_models/gemma-3-27b-it-layer-by-layer"
        if not Path(model_path).exists():
            logger.error(f"‚ùå Model path not found: {model_path}")
            return False
        
        # Initialize HMA pipeline
        pipeline = PureHardwarePipelineHMA()
        
        if not pipeline.initialize(model_path):
            logger.error("‚ùå Failed to initialize HMA pipeline")
            return False
        
        # Show memory distribution
        distribution = get_memory_distribution()
        logger.info(f"üéØ Model Memory Distribution:")
        logger.info(f"   VRAM: {distribution.get('vram_layers', 0)} layers (fastest)")
        logger.info(f"   GTT: {distribution.get('gtt_layers', 0)} layers (fast)")
        logger.info(f"   RAM: {distribution.get('cpu_layers', 0)} layers (fallback)")
        
        # Show final GPU memory state
        gpu_stats = get_gpu_memory_stats()
        if gpu_stats:
            logger.info(f"üìä Final GPU Memory:")
            logger.info(f"   VRAM: {gpu_stats.get('vram_used_gb', 0):.1f}/{gpu_stats.get('vram_total_gb', 0):.1f}GB")
            logger.info(f"   GTT: {gpu_stats.get('gtt_used_gb', 0):.1f}/{gpu_stats.get('gtt_total_gb', 0):.1f}GB")
        
        logger.info("üéâ HMA GPU MODEL LOADING COMPLETE!")
        logger.info("‚úÖ Model distributed across GPU memory hierarchy")
        
        model_loaded = True
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Model loading failed: {e}")
        logger.error(traceback.format_exc())
        return False

async def generate_hma_response(prompt: str, max_tokens: int, temperature: float, top_p: float) -> Dict[str, Any]:
    """Generate response using HMA pipeline"""
    
    if not model_loaded or not pipeline:
        raise RuntimeError("Model or pipeline not loaded")
    
    logger.info("üöÄ HMA GPU GENERATION STARTING")
    logger.info(f"üìù Prompt: {prompt[:100]}...")
    
    # Show memory access pattern
    distribution = get_memory_distribution()
    logger.info(f"üìä Accessing: {distribution.get('vram_layers', 0)} VRAM + {distribution.get('gtt_layers', 0)} GTT + {distribution.get('cpu_layers', 0)} RAM layers")
    
    start_time = time.time()
    
    try:
        # Note: This is a placeholder - you'll need to implement actual generation
        # using the layer access methods from PureHardwarePipelineHMA
        
        # For now, return a mock response showing HMA is working
        response_text = f"[HMA Response: Model loaded across VRAM ({distribution.get('vram_layers', 0)} layers), GTT ({distribution.get('gtt_layers', 0)} layers), and RAM ({distribution.get('cpu_layers', 0)} layers)]"
        
        total_time = time.time() - start_time
        
        return {
            "response": response_text,
            "tokens_generated": 50,
            "generation_time": total_time,
            "tokens_per_second": 50 / total_time if total_time > 0 else 0,
            "memory_distribution": distribution
        }
        
    except Exception as e:
        logger.error(f"‚ùå HMA generation failed: {e}")
        raise RuntimeError(f"Generation failed: {e}")

@app.on_event("startup")
async def startup_event():
    """Initialize model on startup"""
    await load_model()

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    process = psutil.Process()
    memory_info = process.memory_info()
    
    gpu_stats = get_gpu_memory_stats()
    distribution = get_memory_distribution()
    
    return {
        "status": "healthy" if model_loaded else "loading",
        "model_loaded": model_loaded,
        "framework": "Pure Hardware with HMA GPU Memory",
        "acceleration": "Direct Vulkan + NPU with VRAM/GTT allocation",
        "memory_stats": {
            "process_rss_gb": memory_info.rss / (1024**3),
            "vram_used_gb": gpu_stats.get('vram_used_gb', 0),
            "vram_total_gb": gpu_stats.get('vram_total_gb', 0),
            "gtt_used_gb": gpu_stats.get('gtt_used_gb', 0),
            "gtt_total_gb": gpu_stats.get('gtt_total_gb', 0),
        },
        "model_distribution": distribution,
        "pipeline_type": "pure_hardware_hma"
    }

@app.get("/v1/models")
async def list_models():
    """List available models"""
    models = [
        {
            "id": "gemma-3-27b-hma",
            "object": "model",
            "created": int(time.time()),
            "owned_by": "pure-hardware-hma",
            "description": "Gemma 3 27B with HMA GPU memory allocation (VRAM/GTT)"
        }
    ]
    return {"object": "list", "data": models}

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """OpenAI v1 compatible chat completions"""
    try:
        if not model_loaded:
            raise HTTPException(status_code=503, detail="Model not loaded")
        
        # Extract prompt
        prompt = ""
        for message in request.messages:
            prompt += f"{message.role}: {message.content}\n"
        prompt += "assistant: "
        
        # Generate response
        result = await generate_hma_response(
            prompt, 
            request.max_tokens, 
            request.temperature,
            request.top_p
        )
        
        # Format response
        response = {
            "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": result["response"]
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": len(prompt),
                "completion_tokens": result["tokens_generated"],
                "total_tokens": len(prompt) + result["tokens_generated"]
            },
            "hma_stats": {
                "generation_time": result["generation_time"],
                "tokens_per_second": result["tokens_per_second"],
                "memory_distribution": result["memory_distribution"]
            }
        }
        
        return response
        
    except Exception as e:
        logger.error(f"‚ùå Chat completion failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Pure Hardware Gemma 27B API Server with HMA",
        "framework": "No PyTorch/ROCm - Direct HMA GPU Memory",
        "acceleration": "VRAM + GTT + RAM distribution",
        "docs": "/docs",
        "health": "/health"
    }

if __name__ == "__main__":
    print("ü¶Ñ PURE HARDWARE API SERVER WITH HMA GPU MEMORY")
    print("=" * 60)
    print("üöÄ Model loads to VRAM/GTT instead of system RAM")
    print("üéÆ Direct Vulkan iGPU acceleration")
    print("‚ö° Direct NPU kernel acceleration") 
    print("üíæ HMA: VRAM + GTT + RAM hierarchy")
    print("üì° Server: http://localhost:8007")
    print("üõë Press Ctrl+C to stop")
    print()
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8007,
        log_level="info"
    )