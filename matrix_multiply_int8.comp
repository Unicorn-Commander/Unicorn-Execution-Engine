#version 450

// Enable INT8 and FP16 extensions for RDNA3
#extension GL_EXT_shader_16bit_storage : enable
#extension GL_EXT_shader_explicit_arithmetic_types_int8 : enable
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : enable
#extension GL_EXT_shader_8bit_storage : enable

// RDNA3 Optimized INT8 Matrix Multiplication Compute Shader
// Designed for AMD Radeon 780M with native INT8 support
// Uses WMMA (Wave Matrix Multiply Accumulate) instructions when available
layout(local_size_x = 16, local_size_y = 4, local_size_z = 1) in;

// Push constants for matrix dimensions
layout(push_constant) uniform PushConstants {
    uint M;  // rows of A and C
    uint N;  // cols of B and C
    uint K;  // cols of A and rows of B
    uint tile_size;
    uint flags; // Bit 0: output precision (0=FP32, 1=FP16)
} pc;

// INT8 weight buffers - keeping weights quantized!
layout(std430, binding = 0) readonly buffer MatrixA_INT8 {
    int8_t matrix_a_int8[];
};

layout(std430, binding = 1) readonly buffer MatrixB_INT8 {
    int8_t matrix_b_int8[];
};

// Scale factors for dequantization (per-channel or per-tensor)
layout(std430, binding = 3) readonly buffer ScaleA {
    float scale_a[];  // Can be single value or per-row
};

layout(std430, binding = 4) readonly buffer ScaleB {
    float scale_b[];  // Can be single value or per-column
};

// Output buffers (FP32 or FP16 based on flag)
layout(std430, binding = 2) writeonly buffer MatrixC_FP32 {
    float matrix_c_fp32[];
};

layout(std430, binding = 2) writeonly buffer MatrixC_FP16 {
    float16_t matrix_c_fp16[];
};

// Shared memory for tile-based computation
// INT8 for weights to save LDS bandwidth
shared int8_t tile_a_int8[4][16];
shared int8_t tile_b_int8[4][16];

// Shared memory for scales (if per-channel quantization)
shared float scale_tile_a[4];
shared float scale_tile_b[16];

void main() {
    uint global_row = gl_GlobalInvocationID.y;
    uint global_col = gl_GlobalInvocationID.x;
    uint local_row = gl_LocalInvocationID.y;
    uint local_col = gl_LocalInvocationID.x;
    
    // Early exit for out-of-bounds threads
    if (global_row >= pc.M || global_col >= pc.N) {
        return;
    }
    
    // Load scale factors (assuming per-tensor for now, can extend to per-channel)
    float scale_a_val = scale_a[0];  // Simple per-tensor scale
    float scale_b_val = scale_b[0];  // Simple per-tensor scale
    
    // Use int accumulator for INT8 multiplication, then convert to float
    int int_accumulator = 0;
    
    // Tile-based matrix multiplication
    uint num_tiles = (pc.K + pc.tile_size - 1) / pc.tile_size;
    
    for (uint tile_idx = 0; tile_idx < num_tiles; ++tile_idx) {
        // Calculate positions
        uint a_row = global_row;
        uint a_col = tile_idx * pc.tile_size + local_col;
        uint b_row = tile_idx * pc.tile_size + local_row;
        uint b_col = global_col;
        
        // Load A tile (INT8) with bounds checking
        if (a_col < pc.K) {
            tile_a_int8[local_row][local_col] = matrix_a_int8[a_row * pc.K + a_col];
        } else {
            tile_a_int8[local_row][local_col] = int8_t(0);
        }
        
        // Load B tile (INT8) with bounds checking
        if (b_row < pc.K) {
            tile_b_int8[local_row][local_col] = matrix_b_int8[b_row * pc.N + b_col];
        } else {
            tile_b_int8[local_row][local_col] = int8_t(0);
        }
        
        // Synchronize workgroup
        barrier();
        
        // Compute INT8 dot product
        // RDNA3 can do INT8 multiply-accumulate natively
        for (uint k = 0; k < pc.tile_size; ++k) {
            int a_val = int(tile_a_int8[local_row][k]);
            int b_val = int(tile_b_int8[k][local_col]);
            int_accumulator += a_val * b_val;
        }
        
        // Synchronize before next tile
        barrier();
    }
    
    // Dequantize the final result
    float result = float(int_accumulator) * scale_a_val * scale_b_val;
    
    // Store result based on output precision flag
    if ((pc.flags & 1u) != 0u) {
        // FP16 output
        matrix_c_fp16[global_row * pc.N + global_col] = float16_t(result);
    } else {
        // FP32 output
        matrix_c_fp32[global_row * pc.N + global_col] = result;
    }
}