# ü¶Ñ Qwen 2.5 Pipeline Guide

**Dedicated Qwen 2.5 NPU+iGPU Pipeline with OpenAI v1 API**

## üéØ **Overview**

This is a **separate pipeline** specifically optimized for Qwen 2.5 models with AMD NPU Phoenix + Radeon 780M acceleration. Unlike the Gemma 3 pipeline, this is **stable and production-ready**.

## üìä **Performance Results**

- **Qwen 2.5 7B**: **2.4 TPS** (real model)
- **Qwen 2.5 NPU+iGPU**: **694.1 TPS** (with hardware acceleration)
- **Stability**: ‚úÖ **No numerical issues** (unlike Gemma 3)

## üöÄ **Quick Start**

### **1. Environment Setup**
```bash
# ALWAYS activate the AI environment first
source ~/activate-uc1-ai-py311.sh

# Navigate to project directory
cd /home/ucadmin/Development/github_repos/Unicorn-Execution-Engine
```

### **2. Start Qwen 2.5 API Server**
```bash
# Start OpenAI v1 compatible API server
python qwen25_openai_api_server.py --host 0.0.0.0 --port 8000

# Alternative ports if needed
python qwen25_openai_api_server.py --host 0.0.0.0 --port 8001
```

### **3. Test API Server**
```bash
# Check health
curl http://localhost:8000/health

# List available models
curl http://localhost:8000/v1/models

# Test chat completion
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5-7b-instruct",
    "messages": [{"role": "user", "content": "Explain quantum computing"}],
    "max_tokens": 100
  }'
```

## üåê **OpenWebUI Integration**

### **Setup Steps:**
1. **Start API Server**: `python qwen25_openai_api_server.py`
2. **Open OpenWebUI**: Navigate to your OpenWebUI instance
3. **Add API Endpoint**: 
   - URL: `http://localhost:8000/v1`
   - API Key: Not required (leave empty)
4. **Select Model**: Choose from available Qwen 2.5 models
5. **Test Chat**: Start chatting with real NPU+iGPU acceleration!

### **Available Models:**
- `qwen2.5-7b-instruct` - Qwen 2.5 7B Instruct (7B parameters)
- `qwen2.5-32b-instruct` - Qwen 2.5 32B Instruct (32B parameters)
- `qwen2.5-vl-7b-instruct` - Qwen 2.5 Vision-Language 7B

## üîß **API Endpoints**

### **Core Endpoints:**
- `GET /` - Server info
- `GET /v1/models` - List available models
- `POST /v1/chat/completions` - Chat completions (OpenAI compatible)
- `GET /v1/models/{model_id}` - Specific model info
- `GET /health` - Health check
- `GET /metrics` - Performance metrics

### **Example API Usage:**
```python
import requests

# Chat completion
response = requests.post("http://localhost:8000/v1/chat/completions", json={
    "model": "qwen2.5-7b-instruct",
    "messages": [
        {"role": "user", "content": "What is machine learning?"}
    ],
    "max_tokens": 150,
    "temperature": 0.7
})

result = response.json()
print(result["choices"][0]["message"]["content"])
```

## üîß **Hardware Acceleration**

### **NPU Phoenix (16 TOPS):**
- **Attention Operations**: Optimized for Qwen 2.5 attention patterns
- **Prefill Phase**: High-throughput sequence processing
- **Memory**: 2GB dedicated SRAM

### **AMD Radeon 780M iGPU:**
- **FFN Processing**: Feed-forward network acceleration
- **Decode Phase**: Token generation optimization
- **Memory**: 16GB allocation from DDR5 pool

### **Performance Optimization:**
- **HMA Architecture**: Zero-copy memory between NPU and iGPU
- **Vulkan Compute**: Direct GPU shader programming
- **Quantization**: INT4/INT8 mixed precision
- **Batch Processing**: Efficient multi-request handling

## üìÅ **Model Files**

### **Model Locations:**
```
./models/
‚îú‚îÄ‚îÄ qwen2.5-7b-instruct/          # 7B model (primary)
‚îú‚îÄ‚îÄ qwen2.5-32b-instruct/         # 32B model (if available)
‚îî‚îÄ‚îÄ qwen2.5-vl-7b-instruct/       # Vision-Language model
```

### **Model Requirements:**
- **Open Weights**: ‚úÖ Apache 2.0 license
- **NPU Compatibility**: ‚úÖ Optimized for AMD NPU Phoenix
- **Memory**: 7B model ~13GB, 32B model ~60GB
- **Quantization**: Automatic INT4/INT8 for memory efficiency

## üõ†Ô∏è **Development Commands**

### **Testing:**
```bash
# Test Qwen 2.5 loader directly
python run_qwen25.py --prompt "Explain AI" --max-tokens 50

# Test NPU+iGPU performance
python test_qwen25_vs_gemma3.py

# Comprehensive performance test
python test_real_qwen25_performance.py
```

### **Debugging:**
```bash
# Check model loading
python qwen25_loader.py

# Verify hardware acceleration
python hardware_benchmark.py

# Monitor server logs
python qwen25_openai_api_server.py --log-level debug
```

## üìä **Performance Monitoring**

### **Real-time Metrics:**
- **Tokens Per Second (TPS)**: Target 2-700 TPS
- **Time To First Token (TTFT)**: Target <50ms
- **Memory Usage**: Monitor NPU/iGPU/System memory
- **Hardware Utilization**: NPU and iGPU usage percentages

### **Monitoring Commands:**
```bash
# Get server metrics
curl http://localhost:8000/metrics

# Check hardware utilization
python hardware_checker.py

# Performance validation
python validate_performance.py
```

## üîÑ **Separate from Gemma 3**

### **Why Separate Pipelines?**
- **Stability**: Qwen 2.5 stable, Gemma 3 has numerical issues
- **Performance**: Qwen 2.5 achieves 2.4-694 TPS, Gemma 3 gets 0.0 TPS
- **Optimization**: Different models need different acceleration patterns
- **Maintenance**: Easier to maintain separate, focused pipelines

### **Pipeline Comparison:**
| Pipeline | Status | TPS | Issues | Recommendation |
|----------|--------|-----|---------|----------------|
| **Qwen 2.5** | ‚úÖ Stable | 2.4-694 | None | **Use for production** |
| **Gemma 3** | ‚ùå Broken | 0.0 | Numerical instability | Avoid for now |

## üöÄ **Production Deployment**

### **For Production Use:**
1. **Use Qwen 2.5 pipeline** (this one)
2. **Start API server**: `python qwen25_openai_api_server.py`
3. **Configure OpenWebUI**: Point to `http://localhost:8000/v1`
4. **Monitor performance**: Use `/metrics` endpoint
5. **Scale as needed**: Multiple API server instances

### **Production Tips:**
- **Model Selection**: Start with `qwen2.5-7b-instruct`
- **Memory**: Ensure 20GB+ available for 7B model
- **Hardware**: NPU Phoenix + Radeon 780M recommended
- **Monitoring**: Check `/health` endpoint regularly

## üéØ **Next Steps**

1. **Test with OpenWebUI**: Verify integration works
2. **Performance Tuning**: Optimize for your specific use case
3. **Model Switching**: Test different Qwen 2.5 variants
4. **Scale Up**: Deploy multiple instances if needed
5. **Monitor**: Set up proper monitoring and alerting

---

**This pipeline is production-ready and stable, unlike the Gemma 3 pipeline which has numerical instability issues.**