#!/usr/bin/env python3
"""
Parallel Memory-Efficient Quantization for Gemma 3 27B
Uses ALL CPU cores with smart memory management
"""

import os
import torch
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from pathlib import Path
import json
import time
import gc
import psutil
from typing import Dict, Any, Optional, Tuple, List
from safetensors import safe_open
import threading

# CPU optimization - USE ALL CORES
cpu_count = multiprocessing.cpu_count()
os.environ['OMP_NUM_THREADS'] = str(cpu_count)
os.environ['MKL_NUM_THREADS'] = str(cpu_count)
os.environ['NUMEXPR_NUM_THREADS'] = str(cpu_count)
torch.set_num_threads(cpu_count)

import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def quantize_tensor_fast(tensor_data: Tuple[str, torch.Tensor, str]) -> Dict[str, Any]:
    """Fast quantization of a single tensor using all available CPU power"""
    tensor_name, tensor, scheme = tensor_data
    
    # Apply quantization based on scheme
    if 'int8' in scheme:
        if 'symmetric' in scheme:
            # Symmetric quantization for attention weights (NPU optimized)
            scale = torch.max(torch.abs(tensor)) / 127.0
            quantized = torch.round(tensor / scale).clamp(-127, 127).to(torch.int8)
        else:
            # Asymmetric quantization for embeddings (CPU optimized)
            min_val = torch.min(tensor)
            max_val = torch.max(tensor)
            scale = (max_val - min_val) / 255.0
            zero_point = torch.round(-min_val / scale).clamp(0, 255).to(torch.uint8)
            quantized = torch.round(tensor / scale + zero_point).clamp(0, 255).to(torch.uint8)
            scale = torch.stack([scale, zero_point.float()])
        memory_reduction = 0.75
        
    elif 'int4' in scheme:
        # Grouped INT4 quantization for FFN weights (iGPU optimized)
        group_size = 128
        tensor_flat = tensor.flatten()
        
        # Parallel group processing
        num_groups = (tensor_flat.numel() + group_size - 1) // group_size
        groups = [tensor_flat[i*group_size:(i+1)*group_size] for i in range(num_groups)]
        
        # Process groups in parallel
        quantized_groups = []
        scales = []
        
        for group in groups:
            if group.numel() > 0:
                scale = torch.max(torch.abs(group)) / 7.0
                q_group = torch.round(group / scale).clamp(-7, 7).to(torch.int8)
                quantized_groups.append(q_group)
                scales.append(scale)
        
        # Reconstruct tensor
        quantized_flat = torch.cat(quantized_groups)
        
        # Pad to original size if needed
        if quantized_flat.numel() < tensor.numel():
            padding = torch.zeros(tensor.numel() - quantized_flat.numel(), dtype=torch.int8)
            quantized_flat = torch.cat([quantized_flat, padding])
        
        quantized = quantized_flat.reshape(tensor.shape)
        scale = torch.stack(scales)
        memory_reduction = 0.875
        
    else:
        # FP16 fallback
        quantized = tensor.to(torch.float16)
        scale = torch.tensor(1.0)
        memory_reduction = 0.5
    
    original_size = tensor.numel() * 4  # FP32 bytes
    quantized_size = int(original_size * (1 - memory_reduction))
    
    return {
        'tensor_name': tensor_name,
        'quantized': quantized,
        'scale': scale,
        'scheme': scheme,
        'shape': tensor.shape,
        'original_size': original_size,
        'quantized_size': quantized_size,
        'memory_reduction': memory_reduction
    }

class ParallelMemoryEfficientQuantizer:
    """Parallel quantizer that uses all CPU cores efficiently"""
    
    def __init__(self, model_path: str = "./models/gemma-3-27b-it"):
        self.model_path = Path(model_path)
        self.output_dir = Path("./quantized_models/gemma-3-27b-it-parallel-optimized")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.stats = {
            'total_original_size': 0,
            'total_quantized_size': 0,
            'processed_tensors': 0,
            'peak_memory_mb': 0,
            'processing_times': []
        }
        
        # Memory monitoring
        self.process = psutil.Process()
        self.memory_lock = threading.Lock()
        
        logger.info(f"ğŸš€ Parallel Memory-Efficient Quantizer initialized")
        logger.info(f"ğŸ’» Using {cpu_count} CPU cores for maximum performance")
        
    def monitor_memory(self) -> float:
        """Thread-safe memory monitoring"""
        with self.memory_lock:
            memory_info = self.process.memory_info()
            memory_mb = memory_info.rss / (1024 * 1024)
            self.stats['peak_memory_mb'] = max(self.stats['peak_memory_mb'], memory_mb)
            return memory_mb
    
    def get_quantization_scheme(self, tensor_name: str) -> str:
        """Determine optimal quantization scheme"""
        clean_name = tensor_name.replace("model.language_model.", "model.")
        
        if any(x in clean_name for x in ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'self_attn']):
            return 'int8_symmetric'  # NPU optimized
        elif any(x in clean_name for x in ['gate_proj', 'up_proj', 'down_proj', 'mlp']):
            return 'int4_grouped'     # iGPU optimized  
        else:
            return 'int8_asymmetric'  # CPU optimized
    
    def load_tensor_metadata(self, file_path: Path) -> List[Tuple[str, str]]:
        """Load tensor metadata without loading the actual tensors"""
        tensor_info = []
        
        with safe_open(file_path, framework="pt", device="cpu") as f:
            for tensor_key in f.keys():
                scheme = self.get_quantization_scheme(tensor_key)
                tensor_info.append((tensor_key, scheme))
        
        return tensor_info
    
    def process_tensor_batch(self, file_path: Path, tensor_batch: List[Tuple[str, str]]) -> List[Dict[str, Any]]:
        """Process a batch of tensors in parallel"""
        batch_start = time.time()
        
        # Load tensors for this batch
        tensor_data = []
        with safe_open(file_path, framework="pt", device="cpu") as f:
            for tensor_key, scheme in tensor_batch:
                tensor = f.get_tensor(tensor_key)
                tensor_data.append((tensor_key, tensor, scheme))
        
        # Process tensors in parallel
        with ThreadPoolExecutor(max_workers=cpu_count) as executor:
            results = list(executor.map(quantize_tensor_fast, tensor_data))
        
        # Save results immediately
        for result in results:
            output_file = self.output_dir / f"{file_path.stem}_{result['tensor_name'].replace('/', '_').replace('.', '_')}.pt"
            
            save_data = {
                'tensor': result['quantized'],
                'scale': result['scale'],
                'original_key': result['tensor_name'],
                'scheme': result['scheme'],
                'shape': result['shape']
            }
            
            torch.save(save_data, output_file)
            
            # Update stats
            self.stats['total_original_size'] += result['original_size']
            self.stats['total_quantized_size'] += result['quantized_size']
            self.stats['processed_tensors'] += 1
        
        batch_time = time.time() - batch_start
        self.stats['processing_times'].append(batch_time)
        
        # Force cleanup
        del tensor_data, results
        gc.collect()
        
        return len(tensor_batch)
    
    def process_file_parallel(self, file_path: Path) -> Dict[str, Any]:
        """Process a single file with parallel tensor processing"""
        logger.info(f"ğŸ“‚ Processing {file_path.name}")
        
        # Load tensor metadata
        tensor_info = self.load_tensor_metadata(file_path)
        total_tensors = len(tensor_info)
        
        logger.info(f"   ğŸ“Š Found {total_tensors} tensors")
        
        # Create batches for parallel processing
        # Use smaller batches to keep memory usage low
        batch_size = max(1, min(8, total_tensors // cpu_count))  # Adaptive batch size
        batches = [tensor_info[i:i+batch_size] for i in range(0, total_tensors, batch_size)]
        
        logger.info(f"   ğŸ”§ Processing {total_tensors} tensors in {len(batches)} batches using {cpu_count} cores")
        
        file_stats = {
            'tensors_processed': 0,
            'batches_processed': 0,
            'total_batches': len(batches)
        }
        
        # Process batches
        for batch_idx, batch in enumerate(batches):
            memory_before = self.monitor_memory()
            
            processed_count = self.process_tensor_batch(file_path, batch)
            
            memory_after = self.monitor_memory()
            file_stats['tensors_processed'] += processed_count
            file_stats['batches_processed'] += 1
            
            # Progress update
            progress = (batch_idx + 1) / len(batches) * 100
            if (batch_idx + 1) % max(1, len(batches) // 10) == 0:
                logger.info(f"   ğŸ”„ Progress: {progress:.1f}% - Memory: {memory_after:.1f}MB - Tensors: {file_stats['tensors_processed']}/{total_tensors}")
        
        return file_stats
    
    def quantize_model(self) -> Dict[str, Any]:
        """Quantize the entire model using parallel processing"""
        logger.info("ğŸš€ Parallel Memory-Efficient Quantization for Gemma 3 27B")
        logger.info(f"ğŸ’» Using {cpu_count} CPU cores with smart batching")
        logger.info(f"ğŸ“ Model path: {self.model_path}")
        logger.info(f"ğŸ“ Output path: {self.output_dir}")
        
        start_time = time.time()
        
        # Find all safetensors files
        safetensor_files = list(self.model_path.glob("*.safetensors"))
        if not safetensor_files:
            logger.error("âŒ No safetensors files found!")
            return None
        
        logger.info(f"ğŸ“¦ Found {len(safetensor_files)} safetensors files")
        
        # Process each file
        for file_idx, file_path in enumerate(safetensor_files):
            logger.info(f"ğŸ”„ Processing file {file_idx + 1}/{len(safetensor_files)}")
            
            file_stats = self.process_file_parallel(file_path)
            
            # Progress update
            elapsed = time.time() - start_time
            files_remaining = len(safetensor_files) - file_idx - 1
            eta = (elapsed / (file_idx + 1)) * files_remaining if file_idx > 0 else 0
            
            logger.info(f"   âœ… File complete. Processed {file_stats['tensors_processed']} tensors - ETA: {eta/60:.1f}m")
        
        # Final statistics
        total_time = time.time() - start_time
        total_savings = (self.stats['total_original_size'] - self.stats['total_quantized_size']) / self.stats['total_original_size']
        
        avg_processing_time = sum(self.stats['processing_times']) / len(self.stats['processing_times']) if self.stats['processing_times'] else 0
        
        logger.info("ğŸ‰ PARALLEL QUANTIZATION COMPLETE!")
        logger.info(f"â±ï¸ Total time: {total_time/60:.1f} minutes")
        logger.info(f"ğŸ’» Average batch processing time: {avg_processing_time:.2f}s")
        logger.info(f"ğŸ“Š Tensors processed: {self.stats['processed_tensors']:,}")
        logger.info(f"ğŸ“Š Size: {self.stats['total_original_size']/(1024**3):.2f}GB â†’ {self.stats['total_quantized_size']/(1024**3):.2f}GB")
        logger.info(f"ğŸ“Š Memory reduction: {total_savings:.1%}")
        logger.info(f"ğŸ’¾ Peak memory usage: {self.stats['peak_memory_mb']:.1f}MB")
        logger.info(f"ğŸ¯ Fits in 16GB iGPU: {self.stats['total_quantized_size']/(1024**3) < 16}")
        
        # Save metadata
        metadata = {
            "model": "gemma-3-27b-it",
            "quantization_method": "parallel_memory_efficient",
            "quantization_time_minutes": total_time / 60,
            "cpu_cores_used": cpu_count,
            "original_size_gb": self.stats['total_original_size'] / (1024**3),
            "quantized_size_gb": self.stats['total_quantized_size'] / (1024**3),
            "memory_reduction": total_savings,
            "peak_memory_usage_mb": self.stats['peak_memory_mb'],
            "fits_in_16gb_igpu": self.stats['total_quantized_size']/(1024**3) < 16,
            "tensors_processed": self.stats['processed_tensors'],
            "files_processed": len(safetensor_files),
            "average_batch_time_seconds": avg_processing_time
        }
        
        with open(self.output_dir / "quantization_results.json", "w") as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"ğŸ’¾ Results saved to {self.output_dir}")
        return metadata

def main():
    """Main function"""
    # Check available memory
    memory = psutil.virtual_memory()
    logger.info(f"ğŸ–¥ï¸ Available RAM: {memory.available / (1024**3):.1f} GB")
    logger.info(f"ğŸ’» CPU cores: {cpu_count}")
    
    # Initialize quantizer
    quantizer = ParallelMemoryEfficientQuantizer()
    
    # Run quantization
    result = quantizer.quantize_model()
    
    if result:
        logger.info("âœ… Parallel quantization completed successfully!")
        logger.info(f"ğŸ¯ Performance: {result['tensors_processed']} tensors in {result['quantization_time_minutes']:.1f} minutes")
        logger.info(f"ğŸ’¾ Memory efficiency: Peak {result['peak_memory_usage_mb']:.1f}MB")
        return True
    else:
        logger.error("âŒ Parallel quantization failed!")
        return False

if __name__ == "__main__":
    main()